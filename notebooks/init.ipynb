{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/nikita/anaconda3/envs/ml-diplom/lib/python3.7/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "True\nGeForce GTX 1060 with Max-Q Design\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.utils.data.dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.optim as optimize\n",
    "import image_loader as il\n",
    "import properties as pr\n",
    "import copy\n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "\n",
    "#torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "device = 'cpu'\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "                               \n",
    "print(torch.cuda.is_available())\n",
    "print( torch.cuda.get_device_name(0))\n",
    "\"\"\"\n",
    "r = il.map_inputs_labels(pr.data_inputs_path, pr.data_labels_path)\n",
    "\n",
    "train, validate = il.split_set(r, pr.train_size, pr.test_size)\n",
    "\n",
    "il.prepare_data(train, validate)\n",
    "\n",
    "for i in train:\n",
    "    il.save_data(pr.tensor_train_path, i)\n",
    "for j in validate:\n",
    "    il.save_data(pr.tensor_validate_path, j)\n",
    "\"\"\"\n",
    "\n",
    "train_tensor = il.load_tensors(pr.tensor_train_path)\n",
    "validate_tensor = il.load_tensors(pr.tensor_validate_path)\n",
    "\n",
    "\"\"\"\n",
    "train_tensor = list(map(il.apply_resize, train_tensor))\n",
    "validate_tensor = list(map(il.apply_resize, validate_tensor))\n",
    "\n",
    "for i in train_tensor:\n",
    "    il.save_data(pr.tensor_train_path, i)\n",
    "for j in validate_tensor:\n",
    "    il.save_data(pr.tensor_validate_path, j)\n",
    "\"\"\"\n",
    "\n",
    "train_tensor = list(map(il.remove_empty_mask, train_tensor))\n",
    "validate_tensor = list(map(il.remove_empty_mask, validate_tensor))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_model_parameters_to_update(m:models.AlexNet) -> list:\n",
    "    params = []\n",
    "    for name, param in m.named_parameters():\n",
    "        # maybe here better check require gradient back propagation\n",
    "        # and then update\n",
    "        # print(\"name={}\".format(name))\n",
    "        params.append(param)\n",
    "    return params\n",
    "\n",
    "def set_model_last_layer(m:models.AlexNet) -> models.AlexNet:\n",
    "    num_features = m.classifier[6].in_features\n",
    "    m.classifier[6] = nn.Linear(num_features, pr.labels_number) # here 2 for each label \n",
    "    return m\n",
    "\n",
    "def train_model_single_epoch(m:models.AlexNet, data_loader:torch.utils.data.DataLoader, criterion:nn.CrossEntropyLoss, \n",
    "                             optimizer:Optimizer) -> models.AlexNet:\n",
    "    m.train()\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.type(torch.FloatTensor).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model_result = m(inputs)\n",
    "        #print(model_result)\n",
    "        #print(\"-\" * 20)\n",
    "        #print(labels)\n",
    "        loss = criterion(model_result, labels)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return m\n",
    "\n",
    "def validate_model_single_epoch(m:models.AlexNet, data_loader:torch.utils.data.DataLoader) -> float:\n",
    "    confusion_matrix = torch\\\n",
    "        .tensor([[[0 for _ in range(2)] for _ in range(2)] for _ in range(pr.labels_number)])\\\n",
    "        .to(device)\n",
    "    \n",
    "    m.eval()\n",
    "    for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # trust result\n",
    "            for t in labels:\n",
    "                for idx, v in enumerate(t):\n",
    "                    confusion_matrix[idx][v.item()][v.item()] += 1\n",
    "\n",
    "            model_result = m(inputs)\n",
    "            # all vectors size 5\n",
    "            for i, t in enumerate(model_result):\n",
    "                # all classes\n",
    "                for j, v in enumerate(t):\n",
    "                    dst_0 = abs(v.item())\n",
    "                    dst_1 = abs(1 - v.item())\n",
    "                    trust = labels[i][j].item()\n",
    "                    model_answer = 1 if dst_1 < dst_0 else 0 \n",
    "                    confusion_matrix[j][model_answer][trust] += 1                    \n",
    "    \n",
    "    print([calculate_f_measure(i) for i in confusion_matrix])\n",
    "    return sum([calculate_f_measure(i) for i in confusion_matrix]) / pr.labels_number\n",
    "\n",
    "def calculate_f_measure(conf_matrix:torch.Tensor) -> float:\n",
    "    precision_w = 0\n",
    "    recall_w = 0\n",
    "    sum_all = conf_matrix.sum().item()\n",
    "    \n",
    "    for i, t in enumerate(conf_matrix):\n",
    "        c = 0\n",
    "        p = 0\n",
    "        for j, _ in enumerate(t):\n",
    "            c += conf_matrix[i][j].item()\n",
    "            p += conf_matrix[j][i].item()\n",
    "            if p == 0:\n",
    "                precision_w += 0\n",
    "            else:            \n",
    "                precision_w += ((conf_matrix[i][i].item() * c) / p) /  sum_all\n",
    "        recall_w += conf_matrix[i][i].item() / sum_all\n",
    "    return (2 * (precision_w * recall_w)) / (precision_w + recall_w)\n",
    " \n",
    "\n",
    "def train_model(m:models.AlexNet, train_loader:torch.utils.data.DataLoader, validate_loader:torch.utils.data.DataLoader,\n",
    "                criterion:nn.CrossEntropyLoss, \n",
    "                optimizer:Optimizer, epochs:int) -> models.AlexNet:\n",
    "     best_model_state = copy.deepcopy(model.state_dict())\n",
    "     best_acc = 0.0\n",
    "     for epoch in range(epochs):\n",
    "          print(\"Epoch: {}/{}\".format(epoch, epochs))\n",
    "          print(\"-\" * 10)\n",
    "          m = train_model_single_epoch(m, train_loader, criterion, optimizer)\n",
    "          accurency = validate_model_single_epoch(m, validate_loader)\n",
    "          print(\"acc:{}\".format(accurency))\n",
    "          if accurency > best_acc:\n",
    "               best_acc = accurency\n",
    "               best_model_state = copy.deepcopy(m.state_dict())\n",
    "             \n",
    "     best_model = m.load_state_dict(best_model_state)\n",
    "     print(best_acc)\n",
    "     return best_model\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "### EXECUTE ONLY ONCE!!!!!!!!!!!\n",
    "model = models.alexnet(pretrained=True, progress=False)\n",
    "model = set_model_last_layer(model)\n",
    "\n",
    "parameters = get_model_parameters_to_update(model)\n",
    "data_train = torch.utils.data.DataLoader(il.CustomDataset(train_tensor), batch_size=4, shuffle=True, num_workers=4)\n",
    "data_validate = torch.utils.data.DataLoader(il.CustomDataset(validate_tensor), batch_size=4, shuffle=True, num_workers=4)\n",
    "crit = nn.SmoothL1Loss()\n",
    "optim = optimize.SGD(parameters, lr=0.001, momentum=0.9)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch: 0/10\n----------\n",
      "tensor(0.6407, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.3992, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1331, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1047, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1369, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.2340, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1323, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.2564, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.2272, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.2023, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1083, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1650, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.2090, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1944, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1606, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0920, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0836, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1067, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1286, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0456, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1420, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1227, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1019, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1073, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0980, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1237, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1317, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0849, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0775, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1032, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0905, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0317, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1896, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0413, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0800, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0905, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1413, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1100, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1135, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0593, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0718, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0811, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0955, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1069, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0722, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1101, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1197, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1142, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0625, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0421, grad_fn=<SmoothL1LossBackward>)\n",
      "[0.9892426913523349, 0.9987359198998748, 0.9692307692307692, 0.955640362225097, 0.987597977243995]\nacc:0.9800895439904143\nEpoch: 1/10\n----------\n",
      "tensor(0.0700, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0581, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0917, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0679, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.1409, grad_fn=<SmoothL1LossBackward>)\n",
      "tensor(0.0351, grad_fn=<SmoothL1LossBackward>)\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5d66286bd92f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-7162e5315b51>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(m, train_loader, validate_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     86\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m           \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m           \u001b[0maccurency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"acc:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccurency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-7162e5315b51>\u001b[0m in \u001b[0;36mtrain_model_single_epoch\u001b[0;34m(m, data_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-diplom/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml-diplom/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "train_model(model, data_train, data_validate, crit, optim, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}